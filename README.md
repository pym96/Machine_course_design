# MACHINE LEARNING COURSE DESIGN

## Reinforcement Learning

In reinforcement learning, the agent aims to maixmize its cumulative reward:



maxâˆ‘ð‘¡=1ð‘‡ð”¼ð‘Žð‘¡âˆ¼ðœ‹(ð‘ ð‘¡),ð‘ ð‘¡+1âˆ¼ð‘(ð‘ ð‘¡+1âˆ£ð‘ ð‘¡,ð‘Žð‘¡),ð‘ ð‘¡âˆ¼ð‘(ð‘ )[ð›¾ð‘¡âˆ’1ð‘Ÿ(ð‘ ð‘¡,ð‘Žð‘¡)].



From the perspective of Bellman equation, the caculation of cumulative reward can be also formulated as:



ð‘‰(ð‘ ð‘¡)=ð”¼ð‘Žâˆ¼ðœ‹(ð‘ ð‘¡)[ð‘Ÿ(ð‘ ð‘¡,ð‘Žð‘¡)+ð›¾ð‘‰(ð‘ ð‘¡+1)].



## Policy Iteration Learning

Once a policy, ðœ‹, has been improved using **ð‘£**ðœ‹ to yield a better policy, ðœ‹â€², we can then compute ð‘£ðœ‹â€² and improve it again to yield and even better ðœ‹â€³. We can thus obtain a sequence of monotonically improving policies and value functions:

ðœ‹0âŸ¶Eð‘£ðœ‹0âŸ¶Iðœ‹1âŸ¶Eâ‹¯âŸ¶Iðœ‹â‹†âŸ¶Eð‘£â‹†,



where âŸ¶EâŸ¶E denotes a policy *evaluation*, and âŸ¶IâŸ¶I denotes a policy improvement. This way of finding an optimal policy is called *policy iteration*.

### Pesudo of Policy Iteration Learning

1. **Initialization**

   ![image-20230623190519549](C:\Users\pym66\AppData\Roaming\Typora\typora-user-images\image-20230623190519549.png)

2. **Policy Evaluation**

   ![image-20230623190510757](C:\Users\pym66\AppData\Roaming\Typora\typora-user-images\image-20230623190510757.png)

3. **Policy Improvement**

   ![image-20230623190533600](C:\Users\pym66\AppData\Roaming\Typora\typora-user-images\image-20230623190533600.png)

## Reinforcement Learning

In reinforcement learning, the agent aims to maixmize its cumulative reward:

$$
\max \sum_{t=1}^T \mathbb{E}_{a_t \sim \pi(s_t), s_{t+1} \sim p(s_{t+1} \mid s_t, a_t), s_t \sim p(s)}[ \gamma^{t-1} r(s_t,a_t)].
$$

From the perspective of Bellman equation, the caculation of cumulative reward can be also formulated as:

$$
V(s_t) = \mathbb{E}_{a \sim \pi(s_t)}[r(s_t,a_t) + \gamma V(s_{t+1})].
$$

## Policy Iteration Learning

Once a policy, $\pi$, has been improved using $v_{\pi}$ to yield a better policy, $\pi'$, we can then compute $v_{\pi'}$ and improve it again to yield and even better $\pi''$. We can thus obtain a sequence of monotonically improving policies and value functions:

$$
\pi_0 \stackrel{\text{E}}{\longrightarrow} v_{\pi_0} \stackrel{\text{I}}{\longrightarrow} \pi_1 \stackrel{\text{E}}{\longrightarrow} \dots \stackrel{\text{I}}{\longrightarrow}\pi_{\star}\stackrel{\text{E}}{\longrightarrow}v_{\star},
$$

where $\stackrel{\text{E}}{\longrightarrow}$ denotes a policy *evaluation*, and $\stackrel{\text{I}}{\longrightarrow}$ denotes a policy improvement. This way of finding an optimal policy is called *policy iteration*.

### Pesudo of Policy Iteration Learning

1. **Initialization**

    $V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A}(s)$ arbitrarily for all $s \in \mathcal{S}$

2. **Policy Evaluation**

    Loop:

    > $\Delta \leftarrow 0$  
    > Loop for each $s \in \mathcal{S}$:  
    > > $v \leftarrow V(S)$  
    > > $V(s) \leftarrow \sum_{s',r}p(s', r | s, \pi(s))[r + \gamma V(s')]$  
    > > $\Delta \leftarrow \max(\Delta, |v-V(s)|)$
    
    until $\Delta < \theta$ (a small positive number determining the accuracy of estimation)

3. **Policy Improvement**  

    policy-stable $\leftarrow$ true  
    For each $s \in \mathcal{S}$:
    
    > *old-action* $\leftarrow \pi(s)$  
    > $\pi(s) \leftarrow \arg \max_a\sum_{s', r}p(s', r | s, a)[r + \gamma V(s')]$  
    > If *old-action* $\ne \pi(s)$, then *policy-stable* $\leftarrow$ *false*  
    
    If *policy-stable*, then stop and return $V \sim v_{\star}$ and $\pi \sim \pi_{\star}$; else go to 2

## Reinforcement Learning

In reinforcement learning, the agent aims to maixmize its cumulative reward:

$$
\max \sum_{t=1}^T \mathbb{E}_{a_t \sim \pi(s_t), s_{t+1} \sim p(s_{t+1} \mid s_t, a_t), s_t \sim p(s)}[ \gamma^{t-1} r(s_t,a_t)].
$$

From the perspective of Bellman equation, the caculation of cumulative reward can be also formulated as:

$$
V(s_t) = \mathbb{E}_{a \sim \pi(s_t)}[r(s_t,a_t) + \gamma V(s_{t+1})].
$$

## Policy Iteration Learning

Once a policy, $\pi$, has been improved using $v_{\pi}$ to yield a better policy, $\pi'$, we can then compute $v_{\pi'}$ and improve it again to yield and even better $\pi''$. We can thus obtain a sequence of monotonically improving policies and value functions:

$$
\pi_0 \stackrel{\text{E}}{\longrightarrow} v_{\pi_0} \stackrel{\text{I}}{\longrightarrow} \pi_1 \stackrel{\text{E}}{\longrightarrow} \dots \stackrel{\text{I}}{\longrightarrow}\pi_{\star}\stackrel{\text{E}}{\longrightarrow}v_{\star},
$$

where $\stackrel{\text{E}}{\longrightarrow}$ denotes a policy *evaluation*, and $\stackrel{\text{I}}{\longrightarrow}$ denotes a policy improvement. This way of finding an optimal policy is called *policy iteration*.

### Pesudo of Policy Iteration Learning

1. **Initialization**

    $V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A}(s)$ arbitrarily for all $s \in \mathcal{S}$

2. **Policy Evaluation**

    Loop:

    > $\Delta \leftarrow 0$  
    > Loop for each $s \in \mathcal{S}$:  
    > > $v \leftarrow V(S)$  
    > > $V(s) \leftarrow \sum_{s',r}p(s', r | s, \pi(s))[r + \gamma V(s')]$  
    > > $\Delta \leftarrow \max(\Delta, |v-V(s)|)$
    
    until $\Delta < \theta$ (a small positive number determining the accuracy of estimation)

3. **Policy Improvement**  

    policy-stable $\leftarrow$ true  
    For each $s \in \mathcal{S}$:
    
    > *old-action* $\leftarrow \pi(s)$  
    > $\pi(s) \leftarrow \arg \max_a\sum_{s', r}p(s', r | s, a)[r + \gamma V(s')]$  
    > If *old-action* $\ne \pi(s)$, then *policy-stable* $\leftarrow$ *false*  
    
    If *policy-stable*, then stop and return $V \sim v_{\star}$ and $\pi \sim \pi_{\star}$; else go to 2

## Reinforcement Learning

In reinforcement learning, the agent aims to maixmize its cumulative reward:

$$
\max \sum_{t=1}^T \mathbb{E}_{a_t \sim \pi(s_t), s_{t+1} \sim p(s_{t+1} \mid s_t, a_t), s_t \sim p(s)}[ \gamma^{t-1} r(s_t,a_t)].
$$

From the perspective of Bellman equation, the caculation of cumulative reward can be also formulated as:

$$
V(s_t) = \mathbb{E}_{a \sim \pi(s_t)}[r(s_t,a_t) + \gamma V(s_{t+1})].
$$

## Policy Iteration Learning

Once a policy, $\pi$, has been improved using $v_{\pi}$ to yield a better policy, $\pi'$, we can then compute $v_{\pi'}$ and improve it again to yield and even better $\pi''$. We can thus obtain a sequence of monotonically improving policies and value functions:

$$
\pi_0 \stackrel{\text{E}}{\longrightarrow} v_{\pi_0} \stackrel{\text{I}}{\longrightarrow} \pi_1 \stackrel{\text{E}}{\longrightarrow} \dots \stackrel{\text{I}}{\longrightarrow}\pi_{\star}\stackrel{\text{E}}{\longrightarrow}v_{\star},
$$

where $\stackrel{\text{E}}{\longrightarrow}$ denotes a policy *evaluation*, and $\stackrel{\text{I}}{\longrightarrow}$ denotes a policy improvement. This way of finding an optimal policy is called *policy iteration*.

### Pesudo of Policy Iteration Learning

1. **Initialization**

    $V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A}(s)$ arbitrarily for all $s \in \mathcal{S}$

2. **Policy Evaluation**

    Loop:

    > $\Delta \leftarrow 0$  
    > Loop for each $s \in \mathcal{S}$:  
    >
    > > $v \leftarrow V(S)$  
    > > $V(s) \leftarrow \sum_{s',r}p(s', r | s, \pi(s))[r + \gamma V(s')]$  
    > > $\Delta \leftarrow \max(\Delta, |v-V(s)|)$

    until $\Delta < \theta$ (a small positive number determining the accuracy of estimation)

3. **Policy Improvement**  

    policy-stable $\leftarrow$ true  
    For each $s \in \mathcal{S}$:
    
    > *old-action* $\leftarrow \pi(s)$  
    > $\pi(s) \leftarrow \arg \max_a\sum_{s', r}p(s', r | s, a)[r + \gamma V(s')]$  
    > If *old-action* $\ne \pi(s)$, then *policy-stable* $\leftarrow$ *false*  
    
    If *policy-stable*, then stop and return $V \sim v_{\star}$ and $\pi \sim \pi_{\star}$; else go to 2
